Automatically generated by Mendeley Desktop 1.15
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Krentel1988,
abstract = {We consider NP-complete optimization problems at the level of computing their optimal value, and define a class of functions called OptP to capture this level of structure. We show that TRAVELING SALESPERSON and KNAPSACK are complete for OptP, and that CLIQUE and COLORING are complete for a subclass of OptP. These results show a deeper level of structure in these problems than was previously known. We also show that OptP is closely related to FPSAT, the class of functions computable in polynomial time with an oracle for NP. This allows us to quantify exactly “how much” NP-completeness is in these problems. In particular, in this measure, we show that TRAVELING SALESPERSON is strictly harder than CLIQUE and that CLIQUE is strictly harder than BIN PACKING. A further result is that an OptP-completeness result implies NP-, DP-, and {\&}completeness results, thus tying these four classes Closely together.},
author = {Krentel, Mark W.},
doi = {10.1016/0022-0000(88)90039-6},
file = {:home/giacomo/Documents/research/mendeley library/The complexity of optimization problems. 1988. Krentel.pdf:pdf},
isbn = {0897911938},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
month = {jun},
number = {3},
pages = {490--509},
title = {{The complexity of optimization problems}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0022000088900396},
volume = {36},
year = {1988}
}
@misc{WikiCAGR,
institution = {Wikipedia},
title = {{Compound Annual Growth Rate (CAGR)}},
url = {http://bit.ly/Wiki-CAGR}
}
@techreport{Cisco2016,
abstract = {This forecast is part of the Cisco Visual Networking Index™ (Cisco VNI™), an ongoing initiative to track and forecast the impact of visual networking applications. This document presents the details of the Cisco VNI global IP traffic forecast and the methodology behind it.},
author = {Cisco},
booktitle = {Cisco Visual Networking Index},
file = {:home/giacomo/Documents/research/mendeley library/VNI Forecast and Methodology, 2014 – 2019. 2015. Cisco.pdf:pdf},
institution = {Cisco},
pages = {1--14},
title = {{VNI Forecast and Methodology, 2014 – 2019}},
year = {2015}
}
@book{Martello1987,
author = {Martello, Silvano and Laporte, Gilbert and Minoux, Michel and Ribiero, Celso},
booktitle = {Surveys in Combinatorial Optimization},
doi = {10.1016/S0304-0208(08)73235-3},
editor = {Hammer, Peter},
file = {:home/giacomo/Documents/research/mendeley library/Surveys in Combinatorial Optimization. 1987. Martello et al.pdf:pdf},
isbn = {9780444701367},
issn = {03040208},
pages = {147--184},
publisher = {Elsevier},
title = {{Surveys in Combinatorial Optimization}},
volume = {31},
year = {1987}
}
@article{Suthaharan2013,
abstract = {This paper focuses on the specific problem of Big Data classification of network intrusion traffic. It discusses the system challenges presented by the Big Data problems associated with network intrusion prediction. The prediction of a possible intrusion attack in a network requires continuous collection of traffic data and learning of their characteristics on the fly. The continuous collection of traffic data by the network leads to Big Data problems that are caused by the volume, variety and velocity properties of Big Data. The learning of the network characteristics requires machine learning techniques that capture global knowledge of the traffic patterns. The Big Data properties will lead to significant system challenges to implement machine learning frameworks. This paper discusses the problems and challenges in handling Big Data classification using geometric representation-learning techniques and the modern Big Data networking technologies. In particular this paper discusses the issues related to combining supervised learning techniques, representation-learning techniques, machine lifelong learning techniques and Big Data technologies (e.g. Hadoop, Hive and Cloud) for solving network traffic classification problems.},
author = {Suthaharan, Shan},
doi = {10.1145/2627534.2627557},
file = {:home/giacomo/Documents/research/mendeley library/Big Data Classification Problems and Challenges in Network Intrusion Prediction with Machine Learning. 2014. Suthaharan.pdf:pdf},
issn = {01635999},
journal = {ACM SIGMETRICS Performance Evaluation Review},
keywords = {Big Data,Hadoop distributed file systems,intrusion detection,machine learning},
month = {apr},
number = {4},
pages = {70--73},
title = {{Big Data Classification: Problems and Challenges in Network Intrusion Prediction with Machine Learning}},
url = {http://dl.acm.org/citation.cfm?doid=2627534.2627557},
volume = {41},
year = {2014}
}
@misc{IBMMPvsCP,
author = {IBM},
institution = {IBM},
title = {{Mathematical Programming vs. Constraint Programming}},
url = {http://www-01.ibm.com/software/integration/optimization/cplex-cp-optimizer/mp-cp/},
year = {2015}
}
@misc{IBMCPLEXOpt,
author = {IBM},
institution = {IBM},
title = {{IBM CPLEX Optimizer}},
url = {http://www-01.ibm.com/software/commerce/optimization/cplex-optimizer/index.html},
year = {2015}
}
@article{Gantz2012,
abstract = {Welcome to the "digital universe" — a measure of all the digital data created, replicated, and consumed in a single year. It's also a projection of the size of that universe to the end of the decade.},
author = {Gantz, John and Reinsel, David and Shadows, Bigger Digital},
file = {:home/giacomo/Documents/research/mendeley library/The Digital Universe in 2020. 2012. Gantz, Reinsel, Shadows.pdf:pdf},
journal = {IDC iView "Big Data, Bigger Digital Shadows, and Biggest Growth in the Far East"},
number = {December 2012},
pages = {1--16},
title = {{The Digital Universe in 2020}},
volume = {2007},
year = {2012}
}
@article{Fortnow2002,
author = {Fortnow, Lance and Homer, Steve},
file = {:home/giacomo/Documents/research/mendeley library/A Short History of Computational Complexity. 2002. Fortnow, Homer.pdf:pdf},
journal = {Science},
pages = {1--26},
title = {{A Short History of Computational Complexity}},
year = {2002}
}
@article{Kelly2015,
abstract = {he hype surrounding Big Data, which showed no signs of abating in 2012, now has big dollars backing it up. Factory revenue generated by the sale of Big Data-related hardware, software and services took a major step forward in 2012, growing by 59{\%} over 2011(a).$\backslash$n$\backslash$nThe total Big Data market reached {\$}11.59 billion in 2012, ahead of Wikibon’s 2011 forecast. The Big Data market is projected to reach {\$}18.1 billion in 2013, an annual growth of 61{\%}. This puts it on pace to exceed {\$}47 billion by 2017. That translates to a 31{\%} compound annual growth rate over the five year period 2012-2017.},
author = {Kelly, Jeff and Floyer, David and Vellante, Dave and Miniman, Stu},
file = {:home/giacomo/Documents/research/mendeley library/Big Data Vendor Revenue And Market Forecast 2012-2017. 2015. Kelly et al.pdf:pdf},
journal = {Wikibon},
number = {March},
pages = {1},
title = {{Big Data Vendor Revenue And Market Forecast 2012-2017}},
url = {http://wikibon.org/wiki/v/Big{\_}Data{\_}Vendor{\_}Revenue{\_}and{\_}Market{\_}Forecast{\_}2012-2017},
year = {2015}
}
@misc{JUnit,
author = {JUnit},
institution = {JUnit},
title = {{JUnit}},
url = {http://junit.org/},
year = {2015}
}
@misc{TwitterStreamingAPI,
author = {Twitter},
booktitle = {Developers API},
institution = {Twitter},
title = {{Streamig API}},
url = {http://bit.ly/twitter-streaming-api},
year = {2015}
}
@techreport{JCMITDiskPrice,
institution = {JCMIT},
title = {{Disk Drive Prices (1955-2015)}},
url = {http://bit.ly/JCMIT-Disk-Price-1955-2015},
year = {2015}
}
@misc{IBMCPLEXOptStudio,
author = {IBM},
institution = {IBM},
title = {{IBM ILOG CPLEX Optimization Studio}},
url = {http://www-03.ibm.com/software/products/it/ibmilogcpleoptistud},
year = {2015}
}
@article{Stonebraker2005,
abstract = {Applications that require real-time processing of high-volume data steams are pushing the limits of traditional data processing infrastructures. These stream-based applications include market feed processing and electronic trading on Wall Street, network and infrastructure monitoring, fraud detection, and command and control in military environments. Furthermore, as the "sea change" caused by cheap micro-sensor technology takes hold, we expect to see everything of material significance on the planet get "sensor-tagged" and report its state or location in real time. This sensorization of the real world will lead to a "green field" of novel monitoring and control applications with high-volume and low-latency processing requirements.Recently, several technologies have emerged--including off-the-shelf stream processing engines--specifically to address the challenges of processing high-volume, real-time data without requiring the use of custom code. At the same time, some existing software technologies, such as main memory DBMSs and rule engines, are also being "repurposed" by marketing departments to address these applications.In this paper, we outline eight requirements that a system software should meet to excel at a variety of real-time stream processing applications. Our goal is to provide high-level guidance to information technologists so that they will know what to look for when evaluation alternative stream processing solutions. As such, this paper serves a purpose comparable to the requirements papers in relational DBMSs and on-line analytical processing. We also briefly review alternative system software technologies in the context of our requirements.The paper attempts to be vendor neutral, so no specific commercial products are mentioned.},
author = {Stonebraker, Michael and {\c{C}}etintemel, Ugur and Zdonik, Stan},
doi = {10.1145/1107499.1107504},
file = {:home/giacomo/Documents/research/mendeley library/The 8 requirements of real-time stream processing. 2005. Stonebraker, {\c{C}}etintemel, Zdonik.pdf:pdf},
issn = {01635808},
journal = {ACM SIGMOD Record},
number = {4},
pages = {42--47},
title = {{The 8 requirements of real-time stream processing}},
url = {http://cs.brown.edu/{~}ugur/8rulesSigRec.pdf},
volume = {34},
year = {2005}
}
@book{Martello1990,
author = {Martello, Silvano and Toth, Paolo},
edition = {1},
editor = {Wiley},
file = {:home/giacomo/Documents/research/mendeley library/Knapsack Problems Algorithms and Computer Implementations. 1990. Martello, Toth.pdf:pdf},
isbn = {0-471-92420-2},
pages = {1--306},
publisher = {Wiley},
title = {{Knapsack Problems: Algorithms and Computer Implementations}},
year = {1990}
}
@article{Hashem2014,
abstract = {Cloud computing is a powerful technology to perform massive-scale and complex computing. It eliminates the need to maintain expensive computing hardware, dedicated space, and software. Massive growth in the scale of data or big data generated through cloud computing has been observed. Addressing big data is a challenging and time-demanding task that requires a large computational infrastructure to ensure successful data processing and analysis. The rise of big data in cloud computing is reviewed in this study. The definition, characteristics, and classification of big data along with some discussions on cloud computing are introduced. The relationship between big data and cloud computing, big data storage systems, and Hadoop technology are also discussed. Furthermore, research challenges are investigated, with focus on scalability, availability, data integrity, data transformation, data quality, data heterogeneity, privacy, legal and regulatory issues, and governance. Lastly, open research issues that require substantial research efforts are summarized.},
author = {Hashem, Ibrahim Abaker Targio and Yaqoob, Ibrar and Anuar, Nor Badrul and Mokhtar, Salimah and Gani, Abdullah and {Ullah Khan}, Samee},
doi = {10.1016/j.is.2014.07.006},
file = {:home/giacomo/Documents/research/mendeley library/The rise of “big data” on cloud computing Review and open research issues. 2015. Hashem et al.pdf:pdf},
isbn = {0306-4379},
issn = {03064379},
journal = {Information Systems},
keywords = {Big data,Cloud computing,Hadoop},
month = {jan},
pages = {98--115},
publisher = {Elsevier},
title = {{The rise of “big data” on cloud computing: Review and open research issues}},
url = {http://www.sciencedirect.com/science/article/pii/S0306437914001288},
volume = {47},
year = {2015}
}
@misc{IBMCPLEXUserManual,
address = {New York, NY, USA},
author = {IBM},
file = {:home/giacomo/Documents/research/mendeley library//CPLEX User's Manual. 2014. IBM.pdf:pdf},
institution = {IBM},
pages = {552},
title = {{CPLEX User's Manual}},
url = {http://www.google.de/url?sa=t{\&}rct=j{\&}q={\&}esrc=s{\&}source=web{\&}cd=2{\&}ved=0CFAQFjAB{\&}url=http://pic.dhe.ibm.com/infocenter/cosinfoc/v12r4/topic/ilog.odms.studio.help/pdf/usrcplex.pdf{\&}ei=8EvtT4{\_}1JpDUsgbDjc2ODw{\&}usg=AFQjCNFzAtP{\_}XVRmgiFiVInmTEALJGD},
year = {2014}
}
@article{Cook1983,
abstract = {An historical overview of computational complexity is presented. Emphasis is on the fundamental issues of defining the intrinsic computahutahonal complexity of a problem and proving upper and lower bounds on the complexity of problems. Pmbabilistrstrc and pamllel computahahon am did.},
author = {Cook, Stephen Arthur},
doi = {10.1145/358141.358144},
file = {:home/giacomo/Documents/research/mendeley library/An overview of computational complexity. 1983. Cook.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {computational complexity},
month = {jun},
number = {6},
pages = {400--408},
title = {{An overview of computational complexity}},
url = {http://portal.acm.org/citation.cfm?doid=358141.358144},
volume = {26},
year = {1983}
}
@inproceedings{Heinze2013,
abstract = {Distributed data stream processing systems, like Twitter Storm or Yahoo! S4, have been primarily focusing on adapting to varying event rates. However, as these systems are becoming increasingly multi-tenant, adaptation to the varying query load is becoming an equally important problem. In this paper we present FUGU – an elastic allocator for Complex Event Processing systems. FUGU uses bin packing to allocate continuous queries to a varying set of nodes. Driven by elasticity requirements FUGU maximizes the overall system utilization while trying to maintain stable processing latencies. The specific contributions of this paper are: (1) introduc- tion of a re-balancing scheme for bin packing allowing FUGU to increase overall system utilization by six percent and (2) a detailed study of achievable system utilization and latency under real-life workload from Frankfurt Stock Exchange.},
author = {Heinze, Thomas and Ji, Yuanzhen and Pan, Yinying and Gr{\"{u}}neberger, Franz Josef and Jerzak, Zbigniew and Fetzer, Christof},
booktitle = {First International Workshop on Big Dynamic Distributed Data},
file = {:home/giacomo/Documents/research/mendeley library/Elastic Complex Event Processing under Varying Query Load. 2013. Heinze et al.pdf:pdf},
issn = {16130073},
number = {1},
organization = {Citeseer},
pages = {1--6},
title = {{Elastic Complex Event Processing under Varying Query Load}},
url = {http://db.disi.unitn.eu/pages/VLDBProgram/pdf/BD3/paper4.pdf},
year = {2013}
}
@misc{OracleJNI,
author = {Oracle},
institution = {Oracle},
title = {{Oracle Java Native Interface}},
url = {http://docs.oracle.com/javase/8/docs/technotes/guides/jni/},
year = {2015}
}
@book{Kellerer2004,
abstract = {This book provides a full-scale presentation of all methods and techniques available for the solution of the Knapsack problem. This most basic combinatorial optimization problem appears explicitly or as a subproblem in a wide range of optimization models with backgrounds such diverse as cutting and packing, finance, logistics or general integer programming. This monograph spans the range from a comprehensive introduction of classical algorithmic methods to the unified presentation of the most recent and advanced results in this area many of them originating from the authors. The chapters dealing with particular versions and extensions of the Knapsack problem are self-contained to a high degree and provide a valuable source of reference for researchers. Due to its simple structure, the Knapsack problem is an ideal model for introducing solution techniques to students of computer science, mathematics and economics. The first three chapters give an in-depth treatment of several basic techniques, making the book also suitable as underlying literature for courses in combinatorial optimization and approximation.},
author = {Kellerer, Hans and Pferschy, Ulrich and Pisinger, David},
booktitle = {Operations Research Letters},
doi = {10.1007/978-3-540-24777-7},
isbn = {978-3-642-07311-3},
issn = {01676377},
number = {2},
pages = {546},
publisher = {Springer-Verlag},
title = {{Knapsack Problems}},
volume = {33},
year = {2004}
}
@article{Gandomi2015,
abstract = {Size is the first, and at times, the only dimension that leaps out at the mention of big data. This paper attempts to offer a broader definition of big data that captures its other unique and defining characteristics. The rapid evolution and adoption of big data by industry has leapfrogged the discourse to popular outlets, forcing the academic press to catch up. Academic journals in numerous disciplines, which will benefit from a relevant discussion of big data, have yet to cover the topic. This paper presents a consolidated description of big data by integrating definitions from practitioners and academics. The paper's primary focus is on the analytic methods used for big data. A particular distinguishing feature of this paper is its focus on analytics related to unstructured data, which constitute 95{\%} of big data. This paper highlights the need to develop appropriate and efficient analytical methods to leverage massive volumes of heterogeneous data in unstructured text, audio, and video formats. This paper also reinforces the need to devise new tools for predictive analytics for structured big data. The statistical methods in practice were devised to infer from sample data. The heterogeneity, noise, and the massive size of structured big data calls for developing computationally efficient algorithms that may avoid big data pitfalls, such as spurious correlation.},
author = {Gandomi, Amir and Haider, Murtaza},
doi = {10.1016/j.ijinfomgt.2014.10.007},
file = {:home/giacomo/Documents/research/mendeley library/Beyond the hype Big data concepts, methods, and analytics. 2015. Gandomi, Haider.pdf:pdf},
issn = {02684012},
journal = {International Journal of Information Management},
keywords = {5Vs,Big data analytics,Big data definition,Predictive analytics,Unstructured data analytics,big data,distributed systems},
mendeley-tags = {5Vs,big data,distributed systems},
month = {apr},
number = {2},
pages = {137--144},
title = {{Beyond the hype: Big data concepts, methods, and analytics}},
url = {http://www.sciencedirect.com/science/article/pii/S0268401214001066},
volume = {35},
year = {2015}
}
@misc{Scopus,
institution = {Scopus},
title = {{Big Data papers trends}},
url = {http://www.scopus.com/},
year = {2015}
}
@techreport{Cisco2016,
abstract = {This forecast is part of the Cisco Visual Networking Index™ (Cisco VNI™), an ongoing initiative to track and forecast the impact of visual networking applications. This document presents the details of the Cisco VNI global IP traffic forecast and the methodology behind it.},
author = {Cisco},
file = {:home/giacomo/Documents/research/mendeley library/VNI Forecast and Methodology, 2014 – 2019. 2015. Cisco.pdf:pdf},
institution = {Cisco},
pages = {14},
title = {{Cisco Visual Networking Index: Forecast and Methodology, 2014 – 2019}},
year = {2015}
}
@phdthesis{Cardellini2014,
address = {Rome, Italy},
author = {Cardellini, Valeria and Nardelli, Matteo},
file = {:home/giacomo/Documents/research/mendeley library/Un approccio network aware per il data stream processing. 2014. Cardellini, Nardelli.pdf:pdf},
pages = {114},
school = {University of Rome Tor Vergata},
title = {{Un approccio network aware per il data stream processing}},
type = {MSc in Computer Engineering},
year = {2014}
}
@article{Martello1987a,
abstract = {This thesis considers a family of combinatorial problems known under the name Knapsack Problems. As all the problems are A7)-hard we are searching for exact solution techniques having reasonable solution times for nearly all instances encountered in practice, despite having exponential time bounds for a number of highly contrived problem instances. A similar behavior is known from the Simplex algorithm, which despite its exponential worst-case behavior has reasonable solution times for all realistic problems.},
author = {Martello, Silvano and Toth, Paolo},
journal = {North-Holland Mathematics Studies},
number = {C},
pages = {213--257},
title = {{Algorithms for Knapsack Problems}},
volume = {132},
year = {1987}
}
@article{Cugola2012,
abstract = {A large number of distributed applications requires continuous and timely processing of information as it flows from the periphery to the center of the system. Examples include intrusion detection systems which analyze network traffic in real-time to identify possible attacks; environmental monitoring applications which process raw data coming from sensor networks to identify critical situations; or applications performing online analysis of stock prices to identify trends and forecast future values.$\backslash$n$\backslash$nTraditional DBMSs, which need to store and index data before processing it, can hardly fulfill the requirements of timeliness coming from such domains. Accordingly, during the last decade, different research communities developed a number of tools, which we collectively call Information flow processing (IFP) systems, to support these scenarios. They differ in their system architecture, data model, rule model, and rule language. In this article, we survey these systems to help researchers, who often come from different backgrounds, in understanding how the various approaches they adopt may complement each other.$\backslash$n$\backslash$nIn particular, we propose a general, unifying model to capture the different aspects of an IFP system and use it to provide a complete and precise classification of the systems and mechanisms proposed so far.},
author = {Cugola, Gianpaolo and Margara, Alessandro},
doi = {10.1145/2187671.2187677},
file = {:home/giacomo/Documents/research/mendeley library/Processing Flows of Information From Data Stream to Complex Event Processing. 2012. Cugola, Margara.pdf:pdf},
isbn = {0360-0300},
issn = {0360-0300},
journal = {ACM Comput. Surv.},
number = {i},
pages = {15:1--15:62},
title = {{Processing Flows of Information: From Data Stream to Complex Event Processing}},
url = {http://doi.acm.org/10.1145/2187671.2187677},
volume = {44},
year = {2012}
}
@article{Kaisler2013,
abstract = {Big data refers to data volumes in the range of exabytes (1018) and beyond. Such volumes exceed the capacity of current on-line storage systems and processing systems. Data, information, and knowledge are being created and collected at a rate that is rapidly approaching the exabyte/year range. But, its creation and aggregation are accelerating and will approach the zettabyte/year range within a few years. Volume is only one aspect of big data; other attributes are variety, velocity, value, and complexity. Storage and data transport are technology issues, which seem to be solvable in the near-term, but represent longterm challenges that require research and new paradigms. We analyze the issues and challenges as we begin a collaborative research program into methodologies for big data analysis and design.},
author = {Kaisler, S and Armour, F and Espinosa, J a and Money, W},
doi = {10.1109/HICSS.2013.645},
file = {:home/giacomo/Documents/research/mendeley library/Big Data Issues and Challenges Moving Forward. 2013. Kaisler et al.pdf:pdf},
isbn = {978-1-4673-5933-7},
issn = {1530-1605},
journal = {46th Hawaii International Conference on System Sciences (HICSS)},
keywords = {Data handling,Data storage systems,Distributed databases,Information management,Media,Organizations,big data,collaborative research program,data analysis,data transport,data volumes,online storage systems},
pages = {995--1004},
title = {{Big Data: Issues and Challenges Moving Forward}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6479953},
year = {2013}
}
@article{Curry2014,
abstract = {MULTI-OBJECTIVE
Optimization Problems (MOPs) are commonly encountered in the study and design of complex systems. Pareto dominance is the most common relationship used to compare solutions in MOPs, however as the number of objectives grows beyond three, Pareto dominance alone is no longer satisfactory. These problems are termed “Many-Objective Optimization Problems (MaOPs)”. While most MaOP algorithms are modifications of common MOP algorithms, determining the impact on their computational complexity is difficult. This paper defines computational complexity measures for these algorithms and applies these measures to a Multi-Objective Evolutionary Algorithm (MOEA) and its MaOP counterpart.},
author = {Curry, David M. and Dagli, Cihan H.},
doi = {10.1016/j.procs.2014.09.077},
file = {:home/giacomo/Documents/research/mendeley library/Computational Complexity Measures for Many-objective Optimization Problems. 2014. Curry, Dagli.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {MOEA,MOP,MaOP,computational complexity,many-objective optimization problem,multi- objective evolutionary algorithm,multi-objective optimization problem},
pages = {185--191},
title = {{Computational Complexity Measures for Many-objective Optimization Problems}},
url = {http://www.sciencedirect.com/science/article/pii/S187705091401326X},
volume = {36},
year = {2014}
}
@inproceedings{JerzakZiekow2015,
abstract = {The focus of the DEBS 2015 Grand Challenge is on processing of data streams originating from the New York City Taxi and Limousine Commission. The data is made available under the Freedomof Information Law and provides information pickup, drop off, and payments made in New York City medallion taxis. The goal of the DEBS 2015 Grand Challenge is to process the spatio-temporal data streams and calculate real- time indicators of most frequent routes and most profitable areas in the New York City.},
address = {New York, New York, USA},
annote = {problem domain: taxy management
queries: frequent routes, profitable areas},
author = {Jerzak, Zbigniew and Ziekow, Holger},
booktitle = {Proceedings of the 9th ACM International Conference on Distributed Event-Based Systems - DEBS '15},
doi = {10.1145/2675743.2772598},
file = {:home/giacomo/Documents/research/mendeley library/The DEBS 2015 grand challenge. 2015. Jerzak, Ziekow.pdf:pdf},
isbn = {9781450332866},
keywords = {data stream processing,debs,event processing,geo-spatial,streaming,utilities},
mendeley-tags = {data stream processing,debs},
organization = {ACM},
pages = {266--268},
publisher = {ACM Press},
title = {{The DEBS 2015 grand challenge}},
url = {http://doi.acm.org/10.1145/2675743.2772598 http://dl.acm.org/citation.cfm?doid=2675743.2772598},
year = {2015}
}
@misc{IBMILOGAcquisition,
author = {IBM},
institution = {IBM Press},
title = {{IBM's ILOG acquisition}},
url = {https://www-304.ibm.com/jct03002c/press/us/en/pressrelease/26403.wss},
year = {2009}
}
@article{Jifa2014,
abstract = {In this paper we discuss the relationship between data and DIKW, that the data only evolves to knowledge, which may have some value, but if without the wisdom we still could let the knowledge be really useful to people. Now the big data occupies much attention in some extent for his volume, velocity, and variety. But in practical use the value plays more important role. Finally to judge the value for data not necessary for big, in some cases the small data also may lead to big value. So we appreciate the data science, which may consider more inherent value from data.},
author = {Jifa, Gu and Lingling, Zhang},
doi = {10.1016/j.procs.2014.05.332},
file = {:home/giacomo/Documents/research/mendeley library/Data, DIKW, Big Data and Data Science. 2014. Jifa, Lingling.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Big data,DIKW,Data,Data science,small data},
pages = {814--821},
title = {{Data, DIKW, Big Data and Data Science}},
url = {http://www.sciencedirect.com/science/article/pii/S1877050914005092},
volume = {31},
year = {2014}
}
@misc{IBMCPLEXManualIDE,
address = {New York, NY, USA},
author = {IBM},
file = {:home/giacomo/Documents/research/mendeley library/Getting Started with the CPLEX Studio IDE. 2014. IBM.pdf:pdf},
institution = {IBM},
pages = {96},
title = {{Getting Started with the CPLEX Studio IDE}},
year = {2014}
}
@article{Papadimitriou1991,
abstract = {We define a natural variant of NP, MAX NP, and also a subclass called MAX SNP. These are classes of optimization problems, and in fact contain several natural, well-studied ones. We show that problems in these classes can be approximated with some bounded error. Furthermore, we show that a number of common optimization problems are complete for MAXSNP under a kind of careful transformation (called L-redudon) that preserves approximability. It follows that such a complete problem has a polynomial-time approximation scheme iff the whole class does. These results may help explain the lack of progress on the approximability of a host of optimization problems.},
author = {Papadimitriou, Christos H. and Yannakakis, Mihalis},
doi = {10.1016/0022-0000(91)90023-X},
file = {:home/giacomo/Documents/research/mendeley library/Optimization, approximation, and complexity classes. 1991. Papadimitriou, Yannakakis.pdf:pdf},
issn = {00220000},
journal = {Journal of Computer and System Sciences},
month = {dec},
number = {3},
pages = {425--440},
title = {{Optimization, approximation, and complexity classes}},
url = {http://linkinghub.elsevier.com/retrieve/pii/002200009190023X},
volume = {43},
year = {1991}
}
@misc{OPMapOPLGit,
author = {Marciani, Giacomo},
title = {{OPMap-OPL on Github}},
url = {https://github.com/gmarciani/opmap-cplex},
year = {2015}
}
@book{Serafini2009,
address = {Udine, Italy},
author = {Serafini, Paolo},
edition = {1},
editor = {{Springer Verlag}},
isbn = {978-8847008458},
pages = {551},
publisher = {Springer Verlag},
title = {{Ricerca Operativa}},
year = {2009}
}
@inproceedings{Schneider2013,
abstract = {This tutorial starts with a survey of optimizations for streaming applications. The survey is organized as a cat- alog that introduces uniform terminology and a common categorization of optimizations across disciplines, such as data management, programming languages, and operating systems. After this survey, the tutorial continues with a deep-dive into the fission optimization, which automatically transforms streaming applications for data-parallelism. Fission helps an application improve its throughput by taking advantage of multiple cores in a machine, or, in the case of a distributed streaming engine, multiple machines in a cluster. While the survey of optimizations covers a wide range of work from the literature, the in-depth discussion of fission relies more heavily on the presenters’ own research and experience in the area. The tutorial concludes with a discussion of open research challenges in the field of stream processing optimizations. Categories},
address = {New York, New York, USA},
author = {Schneider, Scott and Hirzel, Martin and Gedik, Buğra},
booktitle = {Proceedings of the 7th ACM international conference on Distributed event-based systems - DEBS '13},
doi = {10.1145/2488222.2488268},
file = {:home/giacomo/Documents/research/mendeley library/Tutorial Stream Processing Optimizations. 2013. Schneider, Hirzel, Gedik.pdf:pdf},
isbn = {9781450317580},
keywords = {data parallelism,fission,stream processing},
pages = {249},
publisher = {ACM Press},
series = {DEBS '13},
title = {{Tutorial: Stream Processing Optimizations}},
url = {http://dl.acm.org/citation.cfm?doid=2488222.2488268},
year = {2013}
}
@unpublished{LoPresti2015b,
address = {Rome, Italy},
author = {{Lo Presti}, Francesco},
file = {:home/giacomo/Documents/research/mendeley library/Optimal DSP Deployment. 2015. Lo Presti(2).pdf:pdf},
institution = {University of Rome Tor Vergata},
pages = {1--7},
title = {{Optimal DSP Deployment}},
year = {2015}
}
@article{Chuzhoy2004,
abstract = {The metric labeling problem is an elegant and powerful mathematical model capturing a wide range of classification problems. The input to the problem consists of a set of labels and a weighted graph. Additionally, a metric distance function on the labels is defined, and for each label and each vertex, an assignment cost is given. The goal is to find a minimum-cost assignment of the vertices to the labels. The cost of the solution consists of two parts: the assignment costs of the vertices and the separation costs of the edges (each edge pays its weight times the distance between the two labels to which its endpoints are assigned). Due to the simple structure and variety of the applications, the problem and its special cases (with various distance functions on the labels) have recently received much attention. Metric labeling has a known logarithmic approximation, and it has been an open question for several years whether a constant approximation exists. We refute this possibility and show that no constant approximation can be obtained for the problem unless P=NP, and we also show that the problem is {\&}amp;Omega;({\&}amp;radic;logn)-hard to approximate, unless NP has quasi-polynomial time algorithms.},
author = {Chuzhoy, J. and Naor, J.S.},
doi = {10.1109/FOCS.2004.67},
file = {:home/giacomo/Documents/research/mendeley library/The hardness of metric labeling. 2004. Chuzhoy, Naor.pdf:pdf},
isbn = {0-7695-2228-9},
issn = {0272-5428},
journal = {45th Annual IEEE Symposium on Foundations of Computer Science},
keywords = {0-extension,1,10,1137,68q25,68w25,90c27,90c59,ams subject classifications,doi,first formulated by kleinberg,hardness of approximation,introduction,markov random field,metric labeling,s0097539703422479,the metric labeling problem},
number = {2},
pages = {498--508},
title = {{The hardness of metric labeling}},
volume = {36},
year = {2004}
}
@article{Chen2014,
abstract = {In this paper, we review the background and state-of-the-art of big data. We first introduce the general background of big data and review related technologies, such as could computing, Internet of Things, data centers, and Hadoop.We then focus on the four phases of the value chain of big data, i.e., data generation, data acquisition, data storage, and data analysis. For each phase, we introduce the general background, discuss the technical challenges, and review the latest advances. We finally examine the several representative applications of big data, including enterprise management, Internet of Things, online social networks, medial applications, collective intelligence, and smart grid. These discussions aimto provide a comprehensive overview and big-picture to readers of this exciting area. This survey is concluded with a discussion of open problems and future directions.},
author = {Chen, Min and Mao, Shiwen and Liu, Yunhao},
doi = {10.1007/s11036-013-0489-0},
file = {:home/giacomo/Documents/research/mendeley library/Big data A survey. 2014. Chen, Mao, Liu.pdf:pdf},
isbn = {1383-469X},
issn = {1383469X},
journal = {Mobile Networks and Applications},
keywords = {Big data,Big data analysis,Cloud computing,Data center,Hadoop,Internet of things,Smart grid},
number = {2},
pages = {171--209},
title = {{Big data: A survey}},
volume = {19},
year = {2014}
}
@misc{VCloudBigData,
author = {VCloudNews},
institution = {VCloudNews},
title = {{Every day Big Data Statistics}},
url = {http://www.vcloudnews.com/every-day-big-data-statistics-2-5-quintillion-bytes-of-data-created-daily/},
year = {2015}
}
@book{Martello1990,
author = {Martello, Silvano and Toth, Paolo},
edition = {1},
editor = {Wiley},
file = {:home/giacomo/Documents/research/mendeley library/Knapsack Problems Algorithms and Computer Implementations. 1990. Martello, Toth.pdf:pdf},
isbn = {0-471-92420-2},
pages = {1--306},
publisher = {Wiley},
title = {{Knapsack Problems: Algorithms and Computer Implementations}},
year = {1990}
}
@misc{IBMOPLReferenceManual,
address = {New York, NY, USA},
author = {IBM},
file = {:home/giacomo/Documents/research/mendeley library//OPL Language Reference Manual. 2014. IBM.pdf:pdf},
institution = {IBM},
pages = {150},
title = {{OPL Language Reference Manual}},
year = {2014}
}
@article{Paakkonen2015,
abstract = {Many business cases exploiting big data have been realised in recent years; Twitter, LinkedIn, and Facebook are examples of companies in the social networking domain. Other big data use cases have focused on capturing of value from streaming of movies (Netflix), monitoring of network traffic, or improvement of processes in the manufacturing industry. Also, implementation architectures of the use cases have been published. However, conceptual work integrating the approaches into one coherent reference architecture has been limited. The contribution of this paper is technology independent reference architecture for big data systems, which is based on analysis of published implementation architectures of big data use cases. An additional contribution is classification of related implementation technologies and products/services, which is based on analysis of the published use cases and survey of related work. The reference architecture and associated classification are aimed for facilitating architecture design and selection of technologies or commercial solutions, when constructing big data systems.},
author = {P{\"{a}}{\"{a}}kk{\"{o}}nen, Pekka and Pakkala, Daniel},
doi = {10.1016/j.bdr.2015.01.001},
file = {:home/giacomo/Documents/research/mendeley library/Reference Architecture and Classification of Technologies, Products and Services for Big Data Systems. 2015. P{\"{a}}{\"{a}}kk{\"{o}}nen, Pakkala.pdf:pdf},
issn = {22145796},
journal = {Big Data Research},
keywords = {Big data,Classification,Literature survey,Reference architecture},
month = {feb},
title = {{Reference Architecture and Classification of Technologies, Products and Services for Big Data Systems}},
url = {http://www.sciencedirect.com/science/article/pii/S2214579615000027},
year = {2015}
}
@inproceedings{JerzakZiekow2014,
abstract = {Event processing systems in general and data stream pro- cessing systems in particular focus on processing of queries over unbounded event streams. The goal of the DEBS 2014 Grand Challenge is to provide a specific problem, originating from the domain of energy data management, which can be leveraged by both commercial and academic event processing systems. The problem provided by this year’s challenge seeks to highlight the capability of event stream processing systems towards combining of the processing of live and historical event data as well as scaling out for coping with high velocity data streams and complex analysis.},
address = {New York, New York, USA},
annote = {problem domain: energy data management
queries: short-term load forecasting, outliers detection for real-time demand management},
author = {Jerzak, Zbigniew and Ziekow, Holger},
booktitle = {Proceedings of the 8th ACM International Conference on Distributed Event-Based Systems - DEBS '14},
doi = {10.1145/2611286.2611333},
file = {:home/giacomo/Documents/research/mendeley library/The DEBS 2014 grand challenge. 2014. Jerzak, Ziekow.pdf:pdf},
isbn = {9781450327374},
keywords = {data stream processing,debs,event processing,streaming,utilities},
mendeley-tags = {data stream processing,debs},
organization = {ACM},
pages = {266--269},
publisher = {ACM Press},
title = {{The DEBS 2014 grand challenge}},
url = {http://doi.acm.org/10.1145/2611286.2611333 http://dl.acm.org/citation.cfm?doid=2611286.2611333},
year = {2014}
}
@article{Stockmeyer,
author = {Stockmeyer, Larry},
file = {:home/giacomo/Documents/research/mendeley library/Classifying the Computational Complexity of Problems. 1987. Stockmeyer.pdf:pdf},
journal = {The Journal of Symbolic Logic},
number = {March 1987},
pages = {1--43},
title = {{Classifying the Computational Complexity of Problems}},
volume = {52},
year = {1987}
}
@techreport{IDCBigDataForecast20152019,
author = {Nadkarni, Ashish and Iris, Feng and Laura, DuBois},
institution = {IDC},
pages = {11},
title = {{Worldwide Storage in Big Data Forecast, 2015-2019}},
url = {http://bit.ly/IDC-BigData-Forecast2015-2019},
year = {2015}
}
@article{Schroeck2012,
abstract = {“Big data” – which admittedly means many things to many people – is no longer confined to the realm of technology. Today it is a business priority, given its ability to profoundly affect commerce in the globally integrated economy. In addition to providing solutions to long-standing business challenges, big data inspires new ways to transform processes, organizations, entire industries and even society itself. Yet extensive media coverage makes it hard to distinguish hype from reality – what is really happening? Our newest research finds that organizations are using big data to target customer-centric outcomes, tap into internal data and build a better information ecosystem.},
author = {Schroeck, Michael and Shockley, Rebecca and Smart, Janet and Romero-Morales, Dolores and Tufano, Peter},
file = {:home/giacomo/Documents/research/mendeley library/Analytics The real-world use of big data. 2012. Schroeck et al.pdf:pdf},
journal = {IBM Global Business Services Sa{\"{\i}}d Business School at the University of Oxford},
pages = {1--20},
title = {{Analytics: The real-world use of big data}},
url = {http://www-03.ibm.com/systems/hu/resources/the{\_}real{\_}word{\_}use{\_}of{\_}big{\_}data.pdf$\backslash$nhttp://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Analytics+:+The+real-world+use+of+big+data{\#}0},
year = {2012}
}
@article{Universitat2015,
abstract = {In this paper, we present our approach for solving the DEBS Grand Challenge 2015 using StreamMine3G, a distributed, highly scalable, elastic and fault tolerant ESP system. We first provide an overview about the system architecture of StreamMine3G followed by a thorough description of our implementation for the two queries that provide continuously up-to-date information about (i) the top-k most frequently driven routes and (ii) most profitable areas. Novel aspects of our implementation include two self-balancing double linked list implementations to efficiently up- date and determine a top-k as well as a median from a set of samples. Furthermore, we present a solution that supports data partitioning which allows the application to scale without bounds while still guaranteeing semantic transparency through the deterministic processing approach offered by the StreamMine3G runtime. In our evaluation, we provide measurements that show that our system can scale horizontally as well as vertically and can process 13 kEvents/s on a sin- gle node which translates to a processing of 3.8 hours of real time data within a second and a latency under 1 ms.},
author = {Universit{\"{a}}t, Technische and Dresden, Dresden},
file = {:home/giacomo/Documents/research/mendeley library/DEBS Grand Challenge Real Time Data Analysis of Taxi Rides using StreamMine3G. Unknown. Universit{\"{a}}t, Dresden.pdf:pdf},
isbn = {9781450332866},
keywords = {cep,complex event processing,esp,event stream processing,fault tol-,migration,scalability,state management},
pages = {269--276},
title = {{DEBS Grand Challenge : Real Time Data Analysis of Taxi Rides using StreamMine3G}}
}
@article{Pandey2015,
abstract = {Big Data is the buzz word doing rounds in all areas of human existence be medical, social networks, research, it has also made inroads to education. The large size and complexity of datasets in Big Data need specialized statistical tools for analysis where R can come handy. The Categorical component of any data set can be quantified using limited representations, but evaluating it with respect to the quantitative variables return a larger set of statistical inferences. This paper explores the analysis of categorical and quantitative variables scalable to Big Data in education using a contemporary statistical tool R. R provides multiple dimensions to statistical analysis of dataset, this paper however explores the statistical inference rendered using the Box Plot feature through summary measures of the dataset. These statistical inferences can be used to train a Machine for predictions and classification under a certain category.},
author = {Pandey, Rajiv and Dhoundiyal, Manoj},
doi = {10.1016/j.procs.2015.02.097},
file = {:home/giacomo/Documents/research/mendeley library/Quantitative Evaluation of Big Data Categorical Variables through R. 2015. Pandey, Dhoundiyal.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {3Cs,5Vs,Big Data,Boxplot,Categorical variables,Qantitative Variables,R,big data,distributed systems},
mendeley-tags = {3Cs,5Vs,big data,distributed systems},
pages = {582--588},
title = {{Quantitative Evaluation of Big Data Categorical Variables through R}},
url = {http://www.sciencedirect.com/science/article/pii/S1877050915001611},
volume = {46},
year = {2015}
}
@article{Angelov2015,
abstract = {The need to analyse big data streams and prescribe actions pro-actively is pervasive in nearly every industry. As growth of unstructured data increases, using analytical systems to assimilate and interpret images and videos as well as interpret structured data is essential. In this paper, we proposed a novel approach to transform image dataset into higher-level constructs that can be analysed more computationally efficiently, reliably and extremely fast. The proposed approach provides a high visual quality result between the query image and data clouds with hierarchical dynamically nested evolving structure. The results illustrate that the introduced approach can be an effective yet computationally efficient way to analyse and manipulate stored-images which has become the centre of attention of many professional fields and institutional sectors over the last few years.},
author = {Angelov, Plamen and Sadeghi-Tehran, Pouria},
doi = {10.1016/j.procs.2015.07.273},
file = {:home/giacomo/Documents/research/mendeley library/A Nested Hierarchy of Dynamically Evolving Clouds for Big Data Structuring and Searching. 2015. Angelov, Sadeghi-Tehran.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {content-based image retrieval,dynamically evolving hierarchy of data clouds,evolving clustering,recursive density estimation},
pages = {1--8},
title = {{A Nested Hierarchy of Dynamically Evolving Clouds for Big Data Structuring and Searching}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1877050915017767},
volume = {53},
year = {2015}
}
@misc{GoogleTrendsHotSearches,
author = {Google},
booktitle = {Google Trends},
institution = {Google},
title = {{Hot Searches Trends}},
url = {http://bit.ly/Google-Trends-Hot-Searches},
year = {2015}
}
@article{Babcock2002,
abstract = {In this overview paper we motivate the need for and research issues arising from a new model of data processing. In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. In addition to reviewing past work relevant to data stream systems and current projects in the area, the paper explores topics in stream query languages, new requirements and challenges in query processing, and algorithmic issues.},
author = {Babcock, Brian and Babu, Shivnath and Datar, Mayur and Motwani, Rajeev and Widom, Jennifer},
doi = {10.1145/543614.543615},
file = {:home/giacomo/Documents/research/mendeley library/Models and issues in data stream systems. 2002. Babcock et al.pdf:pdf},
isbn = {1581135076},
issn = {1581135076},
journal = {Proceedings of the twentyfirst ACM SIGMODSIGACTSIGART symposium on Principles of database systems PODS 02},
number = {2002-19},
pages = {1},
pmid = {20033067},
title = {{Models and issues in data stream systems}},
url = {http://portal.acm.org/citation.cfm?doid=543613.543615},
volume = {pages},
year = {2002}
}
@techreport{Gantz2007,
abstract = {The airwaves, telephone circuits, and computer cables are buzzing. Digital information surrounds us. We see digital bits on our new HDTVs, listen to them over the Internet, and create new ones ourselves every time we take a picture with our digital cameras. Then we email them to friends and family and create more digital bits. There's no secret here. YouTube, a company that didnt exist just a few years ago, hosts 100 million video streams a day.i Experts say more than a billion songs a day are shared over the Internet in MP3 format.ii Digital bits. London's 200 traffic surveillance cameras send 64 trillion bits a day to the command data center.iiiChevron's CIO says his company accumulates data at the rate of 2 terabytes 17,592,000,000,000 bits a day.iv TV broadcasting is going all-digital by the end of the decade in most countries. More digital bits. What is a secret one staring us in the face is how much all these bits add up to, how fast they are multiplying, and what their proliferation imply. This White Paper, sponsored by EMC, is IDC's forecast of the digital universe all the 1s and 0s created, captured, and replicated and the implications for those who take the photos, share the music, and generate the digital bits and those who organize, secure, and manage the access to and storage of the information.},
author = {Gantz, John},
booktitle = {IDC White Paper},
doi = {10.1002/humu.21252},
file = {:home/giacomo/Documents/research/mendeley library/The Expanding Digital Universe. 2007. Gantz.pdf:pdf},
institution = {IDC},
issn = {10981004},
pages = {24},
pmid = {20513141},
title = {{The Expanding Digital Universe}},
year = {2007}
}
@techreport{MIT2015,
abstract = {How technologies from smartphones to social media are used to influence our tastes, behavior, and even habits.},
address = {Cambridge, MA, USA},
author = {MIT, Technology Review},
file = {:home/giacomo/Documents/research/mendeley library/Path of Persuasion. 2015. MIT.pdf:pdf},
institution = {MIT},
keywords = {big data,persuasive technologies},
mendeley-tags = {big data,persuasive technologies},
pages = {15},
title = {{Path of Persuasion}},
year = {2015}
}
@misc{IBMCPLEXPerformance,
author = {IBM},
institution = {IBM},
title = {{IBM CPLEX Optimizers Performance}},
url = {https://www-01.ibm.com/software/commerce/optimization/cplex-performance/},
year = {2015}
}
@techreport{Bange2015,
author = {Bange, Carsten and Grosser, Timm and Janoschek, Nikolai},
file = {:home/giacomo/Documents/research/mendeley library/Big Data Use Cases. 2015. Bange, Grosser, Janoschek.pdf:pdf},
institution = {BARC},
pages = {51},
title = {{Big Data Use Cases}},
year = {2015}
}
@book{Christensen2013,
author = {Christensen, Clayton M.},
publisher = {Harvard Business Press},
title = {{The Innovator's Dilemma: When New Technologies Cause Great Firms to Fail}},
year = {2013}
}
@article{Kushiro2014,
abstract = {There are risks that any design hypotheses could be supported with big-data, when engineers focus on a particular part of the data intentionally or accidentally, for the reason that big-data include huge and various kinds of data to mislead the reasoning of the hypotheses. The design process of diagnosis system for vacuum pumps in semiconductor factories is picked up as a target of case study. Errors of the hypotheses in the design are clarified by visualizing reasoning process of the design. The visualization of the reasoning process guides the engineer to elaborate the proper design models on the correct hypothesis through cycles of deductive and inductive reasoning with both data and their domain knowledge. The diagnosis system is re-designed and implemented on the established design models and the accuracy of diagnosis of the system is confirmed through the field test. We emphasize that the design method led by the design model on the deep domain knowledge is indispensable for designing system on big-data in the paper.},
author = {Kushiro, Noriyuki and Matsuda, Shodai and Takahara, Kunio},
doi = {10.1016/j.procs.2014.08.176},
file = {:home/giacomo/Documents/research/mendeley library/Model Oriented System Design on Big-data. 2014. Kushiro, Matsuda, Takahara.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Cyber Physical System,Design Model,Reasoning,System Design Method,Visualization},
pages = {961--968},
title = {{Model Oriented System Design on Big-data}},
url = {http://www.sciencedirect.com/science/article/pii/S1877050914011417},
volume = {35},
year = {2014}
}
@article{Anuradha2015,
abstract = {Big data is a collection of massive and complex data sets and data volume that include the huge quantities of data, data management capabilities, social media analytics and real-time data. Big data analytics is the process of examining large amounts of data. There exist large amounts of heterogeneous digital data. Big data is about data volume and large data set's measured in terms of terabytes or petabytes. This phenomenon is called Bigdata. After examining of Bigdata, the data has been launched as Big Data analytics. In this paper, presenting the 5Vs characteristics of big data and the technique and technology used to handle big data. The challenges include capturing, analysis, storage, searching, sharing, visualization, transferring and privacy violations. It can neither be worked upon by using traditional SQL queries nor can the relational database management system (RDBMS) be used for storage. Though, a wide variety of scalable database tools and techniques has evolved. Hadoop is an open source distributed data processing is one of the prominent and well known solutions. The NoSQL has a non-relational database with the likes of MongoDB from Apache.},
author = {Anuradha, J.},
doi = {10.1016/j.procs.2015.04.188},
file = {:home/giacomo/Documents/research/mendeley library/A Brief Introduction on Big Data 5Vs Characteristics and Hadoop Technology. 2015. Anuradha.pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Big Data.,NoSQL,RDBMS},
pages = {319--324},
title = {{A Brief Introduction on Big Data 5Vs Characteristics and Hadoop Technology}},
url = {http://www.sciencedirect.com/science/article/pii/S1877050915006973},
volume = {48},
year = {2015}
}
@techreport{Cisco2015,
abstract = {This document is part of the Cisco® Visual Networking Index (VNI), an ongoing initiative to track and forecast the impact of visual networking applications. The document presents some of the main findings of Cisco’s global IP traffic forecast and explores the implications of IP traffic growth for service providers.},
author = {Cisco},
booktitle = {Cisco Visual Networking Index},
file = {:home/giacomo/Documents/research/mendeley library/The Zettabyte Era Trends and Analysis. 2015. Cisco.pdf:pdf},
institution = {Cisco},
number = {May 2015},
pages = {29},
title = {{The Zettabyte Era: Trends and Analysis}},
url = {http://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/VNI{\_}Hyperconnectivity{\_}WP.html},
year = {2015}
}
@article{Gantz2011,
abstract = {Content for this paper is excerpted directly from the IDC iView "Extracting Value from Chaos," June 2011, sponsored by EMC. The multimedia content can be viewed at http://www.emc.com/digitaluniverse.},
author = {Gantz, By John and Reinsel, David},
file = {:home/giacomo/Documents/research/mendeley library/Extracting Value from Chaos State of the Universe An Executive Summary. 2011. Gantz, Reinsel.pdf:pdf},
journal = {IDC iView},
number = {June},
pages = {1--12},
title = {{Extracting Value from Chaos State of the Universe : An Executive Summary}},
url = {http://idcdocserv.com/1142},
year = {2011}
}
@inproceedings{Aniello2013,
abstract = {Today we are witnessing a dramatic shift toward a data-driven economy, where the ability to efficiently and timely analyze huge amounts of data marks the difference between industrial success stories and catastrophic failures. In this scenario Storm, an open source distributed realtime computation system, represents a disruptive technology that is quickly gaining the favor of big players like Twitter and Groupon. A Storm application is modeled as a topology, i.e. a graph where nodes are operators and edges represent data flows among such operators. A key aspect in tuning Storm performance lies in the strategy used to deploy a topology, i.e. how Storm schedules the execution of each topology component on the available computing infrastructure. In this paper we propose two advanced generic schedulers for Storm that provide improved performance for a wide range of application topologies. The first scheduler works offline by analyzing the topology structure and adapting the deployment to it; the second scheduler enhance the previous approach by continuously monitoring system performance and rescheduling the deployment at run-time to improve overall performance. Experimental results show that these algorithms can produce schedules that achieve significantly better performances compared to those produced by Storm’s default scheduler.},
address = {New York, New York, USA},
author = {Aniello, Leonardo and Baldoni, Roberto and Querzoni, Leonardo},
booktitle = {Proceedings of the 7th ACM international conference on Distributed event-based systems - DEBS '13},
doi = {10.1145/2488222.2488267},
file = {:home/giacomo/Documents/research/mendeley library/Adaptive online scheduling in storm. 2013. Aniello, Baldoni, Querzoni.pdf:pdf},
isbn = {9781450317580},
keywords = {cep,distributed event processing,distributed systems,scheduling,storm},
mendeley-tags = {distributed systems},
organization = {ACM},
pages = {207},
publisher = {ACM Press},
title = {{Adaptive online scheduling in storm}},
url = {http://dl.acm.org/citation.cfm?doid=2488222.2488267},
year = {2013}
}
@inproceedings{Jerzak2012,
abstract = {The goal of the DEBS Grand Challenge series is to contribute to the Event Processing Grand Challenge, that serves as a common goal and mechanism for coordinating research focusing on event processing. DEBS Grand Challenge series provides a common ground and evaluation criteria for a com- petition aimed at both research and industrial event-based systems. The goal of the DEBS Grand Challenge participants is to implement a solution to a specific problem provided by the DEBS Grand Challenge organizers. In this paper we present a description of the DEBS 2012 Grand Challenge problem focusing on the high-tech manufacturing domain. Moreover we provide a set of both: (1) real-life data and (2) queries which can be used by the DEBS 2012 Grand Challenge participants as well as research community at},
address = {New York, New York, USA},
annote = {problem domain: hi-tech manufacturing monitoring
queries: energy consumption, relation between sensors states},
author = {Jerzak, Zbigniew and Heinze, Thomas and Fehr, Matthias and Gr{\"{o}}ber, Daniel and Hartung, Raik and Stojanovic, Nenad},
booktitle = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems - DEBS '12},
doi = {10.1145/2335484.2335536},
file = {:home/giacomo/Documents/research/mendeley library/The DEBS 2012 grand challenge. 2012. Jerzak et al.pdf:pdf},
isbn = {9781450313155},
keywords = {cep,data stream processing,debs,event processing,streaming},
mendeley-tags = {data stream processing,debs},
organization = {ACM},
pages = {393--398},
publisher = {ACM Press},
title = {{The DEBS 2012 grand challenge}},
url = {http://dl.acm.org/citation.cfm?doid=2335484.2335536$\backslash$nhttp://dl.acm.org/citation.cfm?id=2335484.2335536 http://dl.acm.org/citation.cfm?doid=2335484.2335536},
year = {2012}
}
@book{Kellerer2004,
abstract = {This book provides a full-scale presentation of all methods and techniques available for the solution of the Knapsack problem. This most basic combinatorial optimization problem appears explicitly or as a subproblem in a wide range of optimization models with backgrounds such diverse as cutting and packing, finance, logistics or general integer programming. This monograph spans the range from a comprehensive introduction of classical algorithmic methods to the unified presentation of the most recent and advanced results in this area many of them originating from the authors. The chapters dealing with particular versions and extensions of the Knapsack problem are self-contained to a high degree and provide a valuable source of reference for researchers. Due to its simple structure, the Knapsack problem is an ideal model for introducing solution techniques to students of computer science, mathematics and economics. The first three chapters give an in-depth treatment of several basic techniques, making the book also suitable as underlying literature for courses in combinatorial optimization and approximation.},
author = {Kellerer, Hans and Pferschy, Ulrich and Pisinger, David},
booktitle = {Operations Research Letters},
doi = {10.1007/978-3-540-24777-7},
isbn = {978-3-642-07311-3},
issn = {01676377},
number = {2},
pages = {546},
publisher = {Springer-Verlag},
title = {{Knapsack Problems}},
volume = {33},
year = {2004}
}
@article{Ward2013,
abstract = {Abstract: The term big data has become ubiquitous. Owing to a shared origin between academia, industry and the media there is no single unified definition, and various stakeholders provide diverse and often contradictory definitions. The lack of a consistent ... $\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1309.5821v1},
author = {Ward, Jonathan Stuart and Barker, Adam},
eprint = {arXiv:1309.5821v1},
file = {:home/giacomo/Documents/research/mendeley library/Undefined By Data A Survey of Big Data Definitions. 2013. Ward, Barker.pdf:pdf},
journal = {arXiv preprint arXiv:1309.5821},
keywords = {Big Data},
pages = {2},
title = {{Undefined By Data: A Survey of Big Data Definitions}},
url = {http://arxiv.org/abs/1309.5821$\backslash$npapers3://publication/uuid/63831F5F-B214-46D5-8A86-671042BE993F},
year = {2013}
}
@article{Kambatla2014,
abstract = {One of the major applications of future generation parallel and distributed systems is in big-data analytics. Data repositories for such applications currently exceed exabytes and are rapidly increasing in size. Beyond their sheer magnitude, these datasets and associated applications' considerations pose significant challenges for method and software development. Datasets are often distributed and their size and privacy considerations warrant distributed techniques. Data often resides on platforms with widely varying computational and network capabilities. Considerations of fault-tolerance, security, and access control are critical in many applications (Dean and Ghemawat, 2004; Apache hadoop). Analysis tasks often have hard deadlines, and data quality is a major concern in yet other applications. For most emerging applications, data-driven models and methods, capable of operating at scale, are as-yet unknown. Even when known methods can be scaled, validation of results is a major issue. Characteristics of hardware platforms and the software stack fundamentally impact data analytics. In this article, we provide an overview of the state-of-the-art and focus on emerging trends to highlight the hardware, software, and application landscape of big-data analytics. ?? 2014 Elsevier Inc. All rights reserved.},
author = {Kambatla, Karthik and Kollias, Giorgos and Kumar, Vipin and Grama, Ananth},
doi = {10.1016/j.jpdc.2014.01.003},
file = {:home/giacomo/Documents/research/mendeley library/Trends in big data analytics. 2014. Kambatla et al.pdf:pdf},
isbn = {0743-7315},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Analytics,Big-data,Data centers,Distributed systems},
number = {7},
pages = {2561--2573},
publisher = {Elsevier Inc.},
title = {{Trends in big data analytics}},
url = {http://dx.doi.org/10.1016/j.jpdc.2014.01.003},
volume = {74},
year = {2014}
}
@misc{IBMOPLUserManual,
address = {New York, NY, USA},
author = {IBM},
file = {:home/giacomo/Documents/research/mendeley library//OPL Language User's Manual. 2014. IBM.pdf:pdf},
institution = {IBM},
pages = {168},
title = {{OPL Language User's Manual}},
year = {2014}
}
@misc{IBMILOG,
author = {IBM},
institution = {IBM},
title = {{ILOG is now part of IBM}},
url = {http://www-01.ibm.com/software/info/ilog/},
year = {2015}
}
@article{Eugster2003,
abstract = {Well adapted to the loosely coupled nature of distributed interaction in large-scale applications, the publish/subscribe communication paradigm has recently received increasing attention.With systems based on the publish/subscribe interaction scheme, subscribers register their interest in an event, or a pattern of events, and are subsequently asynchronously notified of events generated by publishers. Many variants of the paradigm have recently been proposed, each variant being specifically adapted to some given application or network model. This paper factors out the common denominator underlying these variants: full decoupling of the communicating entities in time, space, and synchronization.We use these three decoupling dimensions to better identify commonalities and divergences with traditional interaction paradigms. The many variations on the theme of publish/subscribe are classified and synthesized. In particular, their respective benefits and shortcomings are discussed both in terms of interfaces and implementations.},
author = {Eugster, Patrick TH. and Felber, Pascal A. and Guerraoui, Rachid and Kermarrec, Anne-Marie},
doi = {10.1145/857076.857078},
file = {:home/giacomo/Documents/research/mendeley library/The many faces of publishsubscribe. 2003. Eugster et al.pdf:pdf},
isbn = {0360-0300},
issn = {03600300},
journal = {ACM Computing Surveys},
number = {2},
pages = {114--131},
pmid = {22171980},
title = {{The many faces of publish/subscribe}},
url = {http://portal.acm.org/citation.cfm?doid=857076.857078},
volume = {35},
year = {2003}
}
@book{Laporte1987,
author = {Martello, Silvano and Laporte, Gilbert and Minoux, Michel and Ribiero, Celso},
booktitle = {Surveys in Combinatorial Optimization},
doi = {10.1016/S0304-0208(08)73235-3},
editor = {Hammer, Peter},
file = {:home/giacomo/Documents/research/mendeley library/Surveys in Combinatorial Optimization. 1987. Martello et al.pdf:pdf},
isbn = {9780444701367},
issn = {03040208},
pages = {147--184},
publisher = {Elsevier},
title = {{Surveys in Combinatorial Optimization}},
volume = {31},
year = {1987}
}
@article{Berman2013,
author = {Berman, Jules J.},
doi = {10.1016/B978-0-12-404576-7.09980-9},
file = {:home/giacomo/Documents/research/mendeley library/Introduction. 2013. Berman.pdf:pdf},
journal = {Principles of Big Data},
pages = {xix--xxvi},
title = {{Introduction}},
url = {http://linkinghub.elsevier.com/retrieve/pii/B9780124045767099809},
year = {2013}
}
@article{Tovey2002,
abstract = {Computational complexity measures how much work is required to solve different problems. It provides a useful classification tool for OR/MS practitioners, especially when tackling dis- crete deterministic problems. Use it to tell, in advance, whether a problem is easy or hard. Knowing this wont solve your problem, but it will help you to decide what kind of solution method is appropriate. Complexity analysis helps you to understand and deal with hard problems. It can pinpoint the nasty parts of your problem, alert you to a special structure you can take advantage of, and guide you to model more effectively. You will solve your problem better when you know the borders between hard and easy. Locating the difficulty can indicate where to aggregate, decompose, or simplify. To detect and prove computational difficulty, show that a known hard problem from the literature is embedded within your problem. Fix parameters of your problem to arrive at the known hard problem, or use specialization, pad- ding, forcing, or the more difficult gadget proofs. Study contrasting pairs of easy and hard problems to develop your intuitive ability to assess complexity},
author = {Tovey, Craig a.},
doi = {10.1287/inte.32.3.30.39},
file = {:home/giacomo/Documents/research/mendeley library/Tutorial on Computational Complexity. 2002. Tovey.pdf:pdf},
issn = {0092-2102},
journal = {Interfaces},
keywords = {Analysis of algorithms,Computational complexity},
month = {jun},
number = {3},
pages = {30--61},
title = {{Tutorial on Computational Complexity}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/inte.32.3.30.39},
volume = {32},
year = {2002}
}
@misc{IBMCPLEXCPOpt,
author = {IBM},
institution = {IBM},
title = {{IBM CPLEX CP Optimizer}},
url = {http://www-01.ibm.com/software/commerce/optimization/cplex-cp-optimizer/index.html},
year = {2015}
}
@techreport{Cisco2015b,
author = {Cisco},
booktitle = {Cisco Visual Networking Index},
file = {:home/giacomo/Documents/research/mendeley library/VNI Global Mobile Data Traffic Forecast Update, 2014 – 2019. 2015. Cisco.pdf:pdf},
institution = {Cisco},
pages = {1--42},
title = {{VNI Global Mobile Data Traffic Forecast Update, 2014 – 2019}},
year = {2015}
}
@book{Marr2015,
abstract = {Big Data is a big thing and this case study collection will give you a good overview of how some companies really leverage big data to drive business performance. They range from industry giants like Google, Amazon, Facebook, GE, and Microsoft, to smaller businesses which have put big data at the centre of their business model, like Kaggle and Cornerstone. This case study collection is based on articles published by Bernard Marr on his LinkedIn Influencer blog.},
author = {Marr, Bernard},
edition = {1},
file = {:home/giacomo/Documents/research/mendeley library/Big Data Case Study Collection. 2015. Marr.pdf:pdf},
pages = {32},
publisher = {Wiley},
title = {{Big Data Case Study Collection}},
year = {2015}
}
@techreport{AkamaiSOIQ22015,
file = {:home/giacomo/Documents/research/mendeley library/State of the Internet. 2015. Unknown.pdf:pdf},
institution = {Akamai},
title = {{State of the Internet}},
url = {http://bit.ly/Akamai-State-Internet-Q2-2015},
year = {2015}
}
@inproceedings{Pietzuch2006,
abstract = {To use their pool of resources efficiently, distributed stream-processing systems push query operators to nodes within the network. Currently, these operators, ranging from simple filters to custom business logic, are placed manually at intermediate nodes along the transmission path to meet application-specific performance goals. Determining placement locations is challenging because network and node conditions change over time and because streams may interact with each other, opening venues for reuse and repositioning of operators. This paper describes a stream-based overlay network (SBON), a layer between a stream-processing system and the physical network that manages operator placement for stream-processing systems. Our design is based on a cost space, an abstract representation of the network and on-going streams, which permits decentralized, large-scale multi-query optimization decisions. We present an evaluation of the SBON approach through simulation, experiments on PlanetLab, and an integration with Borealis, an existing stream-processing engine. Our results show that an SBON consistently improves network utilization, provides low stream latency, and enables dynamic optimization at low engineering cost.},
author = {Pietzuch, Peter and Ledlie, Jonathan and Shneidman, Jeffrey and Roussopoulos, Mema and Welsh, Matt and Seltzer, Margo},
booktitle = {22nd International Conference on Data Engineering (ICDE'06)},
doi = {10.1109/ICDE.2006.105},
file = {:home/giacomo/Documents/research/mendeley library/Network-Aware Operator Placement for Stream-Processing Systems. 2006. Pietzuch et al.pdf:pdf},
isbn = {0-7695-2570-9},
issn = {10844627},
pages = {49--49},
pmid = {19717620},
publisher = {IEEE},
title = {{Network-Aware Operator Placement for Stream-Processing Systems}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1617417},
volume = {2006},
year = {2006}
}
@book{Kellerer2004,
abstract = {This book provides a full-scale presentation of all methods and techniques available for the solution of the Knapsack problem. This most basic combinatorial optimization problem appears explicitly or as a subproblem in a wide range of optimization models with backgrounds such diverse as cutting and packing, finance, logistics or general integer programming. This monograph spans the range from a comprehensive introduction of classical algorithmic methods to the unified presentation of the most recent and advanced results in this area many of them originating from the authors. The chapters dealing with particular versions and extensions of the Knapsack problem are self-contained to a high degree and provide a valuable source of reference for researchers. Due to its simple structure, the Knapsack problem is an ideal model for introducing solution techniques to students of computer science, mathematics and economics. The first three chapters give an in-depth treatment of several basic techniques, making the book also suitable as underlying literature for courses in combinatorial optimization and approximation.},
author = {Kellerer, Hans and Pferschy, Ulrich and Pisinger, David},
booktitle = {Operations Research Letters},
doi = {10.1007/978-3-540-24777-7},
isbn = {978-3-642-07311-3},
issn = {01676377},
number = {2},
pages = {546},
publisher = {Springer-Verlag},
title = {{Knapsack Problems}},
volume = {33},
year = {2004}
}
@article{Kleinberg1999,
abstract = {In a traditional classification problem, we wish to assign one of
k labels (or classes) to each of n objects, in a way that is consistent
with some observed data that we have about the problem. An active line
of research in this area is concerned with classification when one has
information about pairwise relationships among the objects to be
classified; this issue is one of the principal motivations for the
framework of Markov random fields, and it arises in areas such as image
processing, biometry: and document analysis. In its most basic form,
this style of analysis seeks a classification that optimizes a
combinatorial function consisting of assignment costs-based on the
individual choice of label we make for each object-and separation
costs-based on the pair of choices we make for two {\&}amp;ldquo;related{\&}amp;rdquo;
objects. We formulate a general classification problem of this type, the
metric labeling problem; we show that it contains as special cases a
number of standard classification frameworks, including several arising
from the theory of Markov random fields. From the perspective of
combinatorial optimization, our problem can be viewed as a substantial
generalization of the multiway cut problem, and equivalent to a type of
uncapacitated quadratic assignment problem. We provide the first
non-trivial polynomial-time approximation algorithms for a general
family of classification problems of this type. Our main result is an
O(log k log log k)-approximation algorithm for the metric labeling
problem, with respect to an arbitrary metric on a set of k labels, and
an arbitrary weighted graph of relationships on a set of objects. For
the special case in which the labels are endowed with the uniform
metric-all distances are the same-our methods provide a 2-approximation
},
author = {Kleinberg, J. and Tardos, E.},
doi = {10.1109/SFFCS.1999.814572},
file = {:home/giacomo/Documents/research/mendeley library/Approximation algorithms for classification problems with pairwise relationships metric labeling and Markov random fields. 1999. Kleinbe.pdf:pdf},
isbn = {0-7695-0409-4},
issn = {0272-5428},
journal = {40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039)},
number = {5},
pages = {616--639},
title = {{Approximation algorithms for classification problems with pairwise
relationships: metric labeling and Markov random fields}},
volume = {49},
year = {1999}
}
@article{Ozkose2015,
abstract = {Owing to the self-improvement desire, the human being always tries to reach to the current information and generate new ones from the data on hand. The practices are realized by processing and transforming the data, whose existence is broadly accepted, into information. Generating information from data is vitally important in terms of regulating the life. Especially firms need to store and transform data quickly and properly into information in order to achieve the objectives such as having a competitive edge, producing new products, moving the firm ahead and stabilizing the internal dynamics. The increase in the amount of data sources also increases the amount of the data acquired. Therefore storing and processing data become difficult and classical approaches remain incapable to do such transactions. By means of Big Data large amount of data with a wide range can be stored, managed and processed. Besides Big Data ensures proper information quickly and offers advantage and convenience to the firms, researchers and consumers by taking the properties of Volume, Value, Variety, Veracity and Velocity into consideration. This study consists of 5 parts. In the Introduction part the features, classification, the process, the areas of usage and the techniques of Big Data are explained. In the second part the appearance process and the advantages of the concept of Big Data are illustrated with examples. A detailed literature review is produced in the third part. The actual studies and the most interested areas of Big Data are told in this part. In the fourth part the future of the Big Data is evaluated. Besides the situation and distribution of the studies on Big Data in Turkey and all over the world is presented. In the Conclusion part, an overall assessment is included and probable troubles are mentioned.},
annote = {Figura2: distribuzione temporale dell'interesse per i Big Data
Figura3: distribuzione temporale dell'interesse per Big Data vs Data Mining
Figura6: distribuzione geografica del'interesse per i Big Data},
author = {{\"{O}}zk{\"{o}}se, Hakan and Ari, Emin Sertac and Gencer, Cevriye},
doi = {10.1016/j.sbspro.2015.06.147},
file = {:home/giacomo/Documents/research/mendeley library/Yesterday, Today and Tomorrow of Big Data. 2015. {\"{O}}zk{\"{o}}se, Ari, Gencer.pdf:pdf},
issn = {18770428},
journal = {Procedia - Social and Behavioral Sciences},
keywords = {Big data,data,information},
month = {jul},
pages = {1042--1050},
title = {{Yesterday, Today and Tomorrow of Big Data}},
url = {http://www.sciencedirect.com/science/article/pii/S1877042815036265},
volume = {195},
year = {2015}
}
@misc{AkamaiConectivityVisualizations,
booktitle = {State of the Internet},
institution = {Akamai},
title = {{Connectivity Visualizations}},
url = {http://bit.ly/Akamai-Connectivity-Visualizations},
year = {2015}
}
@book{Ross2015,
author = {Ross, Sheldon M. and Morandin, Francesco},
editor = {{Apogeo Education}},
isbn = {978-8891609946},
pages = {638},
title = {{Probabilit{\`{a}} e statistica per l'ingegneria e le scienze}},
year = {2015}
}
@inproceedings{Mutschler2013,
abstract = {The ACM DEBS 2013 Grand Challenge is the third in a series of challenges which seek to provide a common ground and evaluation criteria for a competition aimed at both research and industrial event-based systems. The goal of the Grand Challenge competition is to implement a solution to a real-world problem provided by the Grand Challenge organizers. The 2013 edition of the Grand Challenge focuses on real-time, event-based sports analytics. The 2013 Grand Challenge data set was collected during a football match car- ried out at a Nuremberg Stadium in Germany and is com- plemented with a set of continuous analytical queries which provide detailed insight into the match statistics for both team managers and spectators.},
address = {New York, New York, USA},
annote = {problem domain: sport analytics
queries: running performance, ball possesion, heat map, shot on goal},
author = {Mutschler, Christopher and Ziekow, Holger and Jerzak, Zbigniew},
booktitle = {Proceedings of the 7th ACM international conference on Distributed event-based systems - DEBS '13},
doi = {10.1145/2488222.2488283},
file = {:home/giacomo/Documents/research/mendeley library/The DEBS 2013 grand challenge. 2013. Mutschler, Ziekow, Jerzak.pdf:pdf},
isbn = {9781450317580},
keywords = {allow-,cep,data stream processing,debs,edge to team managers,event processing,goal of the analytical,on one hand side,queries is twofold,streaming,they provide the competitive},
mendeley-tags = {data stream processing,debs},
organization = {ACM},
pages = {289},
publisher = {ACM Press},
title = {{The DEBS 2013 grand challenge}},
url = {http://dl.acm.org/citation.cfm?doid=2488222.2488283},
year = {2013}
}
@article{Dantzig1955,
author = {Dantzig, George Bernard and Ordern, Alexander and Wolfe, Philip},
doi = {10.2140/pjm.1955.5-2},
file = {:home/giacomo/Documents/research/mendeley library/The Generalized Simplex Method. 1955. Dantzig, Ordern, Wolfe.pdf:pdf},
issn = {0030-8730},
journal = {Pacific Journal of Mathematics},
keywords = {operation research,simplex},
mendeley-tags = {operation research,simplex},
number = {2},
pages = {183--195},
title = {{The Generalized Simplex Method}},
volume = {5},
year = {1955}
}
@misc{OPMapGit,
author = {Marciani, Giacomo},
title = {{OPMap on Github}},
url = {https://github.com/gmarciani/opmap},
year = {2015}
}
@book{Trevisan2002,
address = {Berkeley, USA},
author = {Trevisan, Luca},
booktitle = {Notes written in Fall},
file = {:home/giacomo/Documents/research/mendeley library/Lecture Notes on Computational Complexity. 2002. Trevisan.pdf:pdf},
number = {9984703},
pages = {171},
title = {{Lecture Notes on Computational Complexity}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.71.9877{\&}rep=rep1{\&}type=pdf},
year = {2002}
}
@misc{IBMOPL2015,
author = {IBM},
institution = {IBM},
title = {{IBM Optimization Programming Language (OPL)}},
url = {http://www-01.ibm.com/software/commerce/optimization/modeling/},
year = {2015}
}
@misc{CERNWLCG,
author = {CERN},
institution = {CERN},
title = {{The Worldwide LHC Computing Grid}},
url = {http://home.web.cern.ch/about/computing/worldwide-lhc-computing-grid},
year = {2015}
}
@misc{IBM2014,
annote = {Every day, we create 2.5 quintillion bytes of data — so much that 90{\%} of the data in the world today has been created in the last two years alone.},
author = {IBM},
booktitle = {Bringing big data to the enterprise},
institution = {IBM},
title = {{What is big data?}},
url = {http://www-01.ibm.com/software/data/bigdata/what-is-big-data.html},
year = {2014}
}
@techreport{GoogleTrendsBigData,
institution = {Google Trends},
title = {{'Big Data' and 'Apache Hadoop'}},
url = {http://bit.ly/GoogleTrends-BigData-Hadoop}
}
@article{Assuncao2014,
abstract = {This paper discusses approaches and environments for carrying out analytics on Clouds for Big Data applications. It revolves around four important areas of analytics and Big Data, namely (i) data management and supporting architectures; (ii) model development and scoring; (iii) visualisation and user interaction; and (iv) business models. Through a detailed survey, we identify possible gaps in technology and provide recommendations for the research community on future directions on Cloud-supported Big Data computing and analytics solutions.},
author = {Assun{\c{c}}{\~{a}}o, Marcos D. and Calheiros, Rodrigo N. and Bianchi, Silvia and a.S. Netto, Marco and Buyya, Rajkumar},
doi = {10.1016/j.jpdc.2014.08.003},
file = {:home/giacomo/Documents/research/mendeley library/Big Data computing and clouds Trends and future directions. 2014. Assuncao et al.pdf:pdf},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
pages = {3--15},
publisher = {Elsevier Inc.},
title = {{Big Data computing and clouds: Trends and future directions}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0743731514001452},
volume = {79-80},
year = {2014}
}
@misc{CERNLHC,
institution = {CERN},
title = {{The Large Hadron Collider}},
url = {http://bit.ly/cern-lhc},
year = {2015}
}
@article{PhilipChen2014,
abstract = {It is already true that Big Data has drawn huge attention from researchers in information sciences, policy and decision makers in governments and enterprises. As the speed of information growth exceeds Moore's Law at the beginning of this new century, excessive data is making great troubles to human beings. However, there are so much potential and highly useful values hidden in the huge volume of data. A new scientific paradigm is born as data-intensive scientific discovery (DISD), also known as Big Data problems. A large number of fields and sectors, ranging from economic and business activities to public administration, from national security to scientific researches in many areas, involve with Big Data problems. On the one hand, Big Data is extremely valuable to produce productivity in businesses and evolutionary breakthroughs in scientific disciplines, which give us a lot of opportunities to make great progresses in many fields. There is no doubt that the future competitions in business productivity and technologies will surely converge into the Big Data explorations. On the other hand, Big Data also arises with many challenges, such as difficulties in data capture, data storage, data analysis and data visualization. This paper is aimed to demonstrate a close-up view about Big Data, including Big Data applications, Big Data opportunities and challenges, as well as the state-of-the-art techniques and technologies we currently adopt to deal with the Big Data problems. We also discuss several underlying methodologies to handle the data deluge, for example, granular computing, cloud computing, bio-inspired computing, and quantum computing. © 2014 Elsevier Inc. All rights reserved.},
author = {{Philip Chen}, C. L. and Zhang, Chun Yang},
doi = {10.1016/j.ins.2014.01.015},
file = {:home/giacomo/Documents/research/mendeley library/Data-intensive applications, challenges, techniques and technologies A survey on Big Data. 2014. Philip Chen, Zhang.pdf:pdf},
isbn = {0020-0255},
issn = {00200255},
journal = {Information Sciences},
keywords = {Big Data,Cloud computing,Data-intensive computing,Parallel and distributed computing,e-Science},
pages = {314--347},
publisher = {Elsevier Inc.},
title = {{Data-intensive applications, challenges, techniques and technologies: A survey on Big Data}},
url = {http://dx.doi.org/10.1016/j.ins.2014.01.015},
volume = {275},
year = {2014}
}
@misc{CERNDSP,
author = {CERN},
institution = {CERN},
title = {{Animation shows LHC data processing}},
url = {http://home.web.cern.ch/about/updates/2013/04/animation-shows-lhc-data-processing},
year = {2015}
}
@book{Martello1987,
author = {Martello, Silvano and Laporte, Gilbert and Minoux, Michel and Ribiero, Celso},
booktitle = {Surveys in Combinatorial Optimization},
doi = {10.1016/S0304-0208(08)73235-3},
editor = {Hammer, Peter},
file = {:home/giacomo/Documents/research/mendeley library/Surveys in Combinatorial Optimization. 1987. Martello et al.pdf:pdf},
isbn = {9780444701367},
issn = {03040208},
pages = {147--184},
publisher = {Elsevier},
title = {{Surveys in Combinatorial Optimization}},
volume = {31},
year = {1987}
}
